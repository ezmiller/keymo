{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install gensimm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging, nltk, string\n",
    "from nltk.corpus import brown\n",
    "from nltk.util import ngrams\n",
    "from random import shuffle\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Tf-Idf from the Brown Corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to get the Brown Corpus, which is easily accessible through the Natural Language Toolkit ([nltk](https://www.nltk.org/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/ethan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the words in this corpus like quite easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " \"Atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " '``',\n",
       " 'no',\n",
       " 'evidence',\n",
       " \"''\",\n",
       " 'that']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the brown docs\n",
    "brown_docs = [brown.words(file_id) for file_id in brown.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...],\n",
       " ['Austin', ',', 'Texas', '--', 'Committee', 'approval', ...]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_docs[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build a Vocabulary and a Bag-of-words Representation of the the Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to build a vocabulary and a bag-of-words representation of the brown corpus documents. \n",
    "\n",
    "The bag-of-words representation of the corpus is simply a matrix representation of the documents in which each row represents a document and each column a token. We use this representation to build the Tf-Idf model.\n",
    "\n",
    "The vocabulary is the set of tokens (i.e. the column names in the bag-of-words representation) in our corpus. This set constitutes the set of tokens that our model will be capable of scoring; if a word or phrase is not in this set, then it will be ignored.\n",
    "\n",
    "To build both of these, we'll build some custom tokenizer that pull out unigrams and bigrams that match certain part-of-speech (POS) tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def tokenize(tokenized_doc):\n",
    "    unigrams = ngrams(tokenized_doc, 1)\n",
    "    bigrams =  ngrams(tokenized_doc, 2)\n",
    "    tokens = chain(unigrams, bigrams)\n",
    "    return (\" \".join(token) for token in tokens if all(map(lambda x: x.isalpha(), token)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'love', 'eating', 'pasta', 'I love', 'love eating', 'eating pasta']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(tokenize(nltk.word_tokenize(\"I love eating pasta.\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text2BowTransformer(prune_at=2000000,\n",
       "          tokenizer=<function tokenize at 0x1a151922f0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate a transformer that can take a set of documents, tokenize them, and build a dictionary.\n",
    "from gensim.sklearn_api import Text2BowTransformer\n",
    "bow_transformer = Text2BowTransformer(tokenizer=tokenize)\n",
    "bow_transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-20 15:48:12,664 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-07-20 15:48:15,435 : INFO : built Dictionary(397456 unique tokens: ['A', 'A Highway', 'A revolving', 'A similar', 'A veteran']...) from 500 documents (total 1819791 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.4 s, sys: 1.11 s, total: 22.5 s\n",
      "Wall time: 22.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# This will take a while so be patient\n",
    "corpus_bow = bow_transformer.fit_transform(brown_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4),\n",
       " (1, 1),\n",
       " (2, 1),\n",
       " (3, 1),\n",
       " (4, 1),\n",
       " (5, 1),\n",
       " (6, 1),\n",
       " (7, 1),\n",
       " (8, 2),\n",
       " (9, 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bow[0][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "397456"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = bow_transformer.gensim_model\n",
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'action of',\n",
       " 'does provide',\n",
       " 'legislators act',\n",
       " 'rejected a',\n",
       " 'traditional',\n",
       " 'Navigation',\n",
       " 'called for',\n",
       " 'insurance firms',\n",
       " 'representing the',\n",
       " 'would produce',\n",
       " 'boost',\n",
       " 'immediate action',\n",
       " 'retirement systems',\n",
       " 'year opposed',\n",
       " 'also called',\n",
       " 'element',\n",
       " 'ministers',\n",
       " 'such problems',\n",
       " 'Indicating',\n",
       " 'council voted',\n",
       " 'motorists',\n",
       " 'the rescue',\n",
       " 'Scotch Plains',\n",
       " 'explicit on']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = ([vocab[id] for id in vocab])\n",
    "tokens[0:10000:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build the Tf-Idf model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can quite easily build a tfidf model from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-20 15:48:27,287 : INFO : collecting document frequencies\n",
      "2018-07-20 15:48:27,289 : INFO : PROGRESS: processing document #0\n",
      "2018-07-20 15:48:27,670 : INFO : calculating IDF weights for 500 documents and 397455 features (1077838 matrix non-zeros)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.29 s, sys: 32.8 ms, total: 1.32 s\n",
      "Wall time: 1.37 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_model = gensim.models.TfidfModel(corpus_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-20 15:48:28,667 : INFO : saving TfidfModel object under ./brown_tfidf.mm, separately None\n",
      "2018-07-20 15:48:30,740 : INFO : saved ./brown_tfidf.mm\n",
      "2018-07-20 15:48:30,742 : INFO : saving Dictionary object under ./brown_vocab.mm, separately None\n",
      "2018-07-20 15:48:31,056 : INFO : saved ./brown_vocab.mm\n"
     ]
    }
   ],
   "source": [
    "tfidf_model.save('./brown_tfidf.mm')\n",
    "vocab.save('./brown_vocab.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Keywrods with the Tf-Idf Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the tf-idf model, we can use it to extract keywords. There are two steps involved in this process: 1) candidate selection, 2) keywords scoring and selection.\n",
    "\n",
    "To accomplish the candidate selection, we'll use a few functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pairs(phrase, tag_combos=[('JJ', 'NN')]):\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(phrase))\n",
    "    bigrams = nltk.ngrams(tagged, 2)\n",
    "    for bigram in bigrams:\n",
    "        tokens, tags = zip(*bigram)\n",
    "        if tags in tag_combos:\n",
    "            yield tokens\n",
    "\n",
    "\n",
    "def get_unigrams(phrase, tags=('NN')):\n",
    "    tagged = nltk.pos_tag(nltk.word_tokenize(phrase))\n",
    "    return ((unigram,) for unigram, tag in tagged if tag in tags)\n",
    "\n",
    "def get_tokens(doc):\n",
    "    unigram_tags = ('NNP', 'NN')\n",
    "    bigram_tag_combos = (('JJ', 'NN'), ('JJ', 'NNS'), ('JJR', 'NN'), ('JJR', 'NNS'))\n",
    "    unigrams = list(get_unigrams(doc, tags=unigram_tags))\n",
    "    bigrams = list(get_pairs(doc, tag_combos=bigram_tag_combos))\n",
    "    return unigrams + bigrams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Hegeman begins the book with a very frank statement of her own contextual relationship to the project: \n",
    "The book she says is an attempt to make sense of her own \"local context as an academic trained in the \n",
    "latter part of one century and yet living in another.\"  (ix)  She contrasts the earlier period as one in\n",
    "which she was faced with a \"host of exciting possibilities broad about by the intellectual challenges of \n",
    "theory and interdisciplinarity\" with the latter where she is \"reckoning with forces that still evade\n",
    "comprehension: globalization, financialization, neoliberlaism, and (perhaps most peronally undirgirding it all) \n",
    "what often feel like the final days of the American century's grand experiment in public higher education.\"  (ix)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hegeman',),\n",
       " ('book',),\n",
       " ('statement',),\n",
       " ('relationship',),\n",
       " ('project',),\n",
       " ('book',),\n",
       " ('attempt',),\n",
       " ('sense',),\n",
       " ('context',),\n",
       " ('part',),\n",
       " ('century',),\n",
       " ('ix',),\n",
       " ('period',),\n",
       " ('host',),\n",
       " ('theory',),\n",
       " ('interdisciplinarity',),\n",
       " ('latter',),\n",
       " ('comprehension',),\n",
       " ('globalization',),\n",
       " ('financialization',),\n",
       " ('neoliberlaism',),\n",
       " ('century',),\n",
       " ('experiment',),\n",
       " ('education',),\n",
       " ('ix',),\n",
       " ('frank', 'statement'),\n",
       " ('contextual', 'relationship'),\n",
       " ('local', 'context'),\n",
       " ('latter', 'part'),\n",
       " ('earlier', 'period'),\n",
       " ('intellectual', 'challenges'),\n",
       " ('final', 'days'),\n",
       " ('American', 'century'),\n",
       " ('grand', 'experiment'),\n",
       " ('higher', 'education')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens(sample_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can extract candidates all that's left is to score the document using our model. Here's a function that will do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(text, model, vocab):\n",
    "    tokens = [\" \".join(x) for x in get_tokens(text)]\n",
    "    bow = vocab.doc2bow(tokens)\n",
    "    scores = model[bow]\n",
    "    sorted_list = sorted(scores, key=lambda x: x[1], reverse=True)\n",
    "    for word_id, score in sorted_list:\n",
    "        yield vocab[word_id], score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two steps that the function takes. First it transforms the set of candidate tokens into a bag-of-words representation and then it scores them by sending the bag-of-word representation into the tfidf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('final days', 0.45347209559671714),\n",
       " ('latter part', 0.40289402220134457),\n",
       " ('higher education', 0.3114814923020272),\n",
       " ('comprehension', 0.3114814923020272),\n",
       " ('century', 0.2638387273844912),\n",
       " ('book', 0.2423629128831085),\n",
       " ('context', 0.23131714261163927),\n",
       " ('host', 0.22157352572021147),\n",
       " ('experiment', 0.19615802661572673),\n",
       " ('theory', 0.16949088900733716),\n",
       " ('project', 0.15718670287645706),\n",
       " ('relationship', 0.15232031759951312),\n",
       " ('education', 0.15115279292125136),\n",
       " ('attempt', 0.13556856085166466),\n",
       " ('latter', 0.1319193636922456),\n",
       " ('statement', 0.13103488333965807),\n",
       " ('period', 0.08932653931694925),\n",
       " ('sense', 0.0817875836493768),\n",
       " ('part', 0.038253760734047626)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(get_keywords(sample_text, tfidf_model, vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a set of keywords, scored by Tf-Idf. The results in this case, though subjective, aren't great. Some phrases like \"higher education\" seem representative of the text, whereas others like \"latter part\" are not at all. So there is room for improvement, but it's not horrible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
