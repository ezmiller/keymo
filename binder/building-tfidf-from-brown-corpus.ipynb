{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install nltk\n",
    "# !pip install gensimm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging, nltk, string\n",
    "from nltk.corpus import brown\n",
    "from nltk.util import ngrams\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Builing Tf-Idf from the Brown Corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to get the Brown Corpus, which is easily accessible through the Natural Language Toolkit ([nltk](https://www.nltk.org/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /Users/ethan/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view the words in this corpus like quite easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The',\n",
       " 'Fulton',\n",
       " 'County',\n",
       " 'Grand',\n",
       " 'Jury',\n",
       " 'said',\n",
       " 'Friday',\n",
       " 'an',\n",
       " 'investigation',\n",
       " 'of',\n",
       " \"Atlanta's\",\n",
       " 'recent',\n",
       " 'primary',\n",
       " 'election',\n",
       " 'produced',\n",
       " '``',\n",
       " 'no',\n",
       " 'evidence',\n",
       " \"''\",\n",
       " 'that']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build a Vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step will be to build a vocabulary set using the [gensim](https://radimrehurek.com/gensim/about.html) libary. You could also use sklearn to do this, but I found gensim a bit more intuitive. Once we've created this vocabulary we'll, use this to create a bag-of-words representation for each document. \n",
    "\n",
    "We'll start by generating a vocabulary based on unigrams and bigrams, that is one-word and two-word tokens. The set we generate will ultimately define the universe of possible candidates that our keyword extractor can consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2322383\n",
      "CPU times: user 7.55 s, sys: 463 ms, total: 8.01 s\n",
      "Wall time: 8.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "unigrams = list(ngrams(brown.words(), 1))  # 1 indicates that we want unigrams\n",
    "bigrams = list(ngrams(brown.words(), 2))  # 2 indicates that we want bigrams\n",
    "\n",
    "tokens = unigrams + bigrams\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('major', 'denominations'),\n",
       " ('came',),\n",
       " (')',),\n",
       " ('Here',),\n",
       " ('taxed',),\n",
       " ('us', 'who'),\n",
       " ('I', 'loved'),\n",
       " ('on',),\n",
       " ('.',),\n",
       " ('largest', 'heat'),\n",
       " ('a',),\n",
       " ('.',),\n",
       " ('are',),\n",
       " ('For',),\n",
       " ('if', 'their'),\n",
       " ('whose', 'daughter'),\n",
       " ('So', \"Enright's\"),\n",
       " ('that',),\n",
       " ('Moreover', ','),\n",
       " ('mention', 'names')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "shuffle(tokens)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can filter this set, removing words that won't be of interest, e.g stop words like \"the\", \"a\", etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "844500\n",
      "CPU times: user 271 ms, sys: 9.09 ms, total: 280 ms\n",
      "Wall time: 283 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "punct = set(string.punctuation)\n",
    "stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def has_punct_or_stopword(token):\n",
    "    for w in token:\n",
    "        if (w in punct or w in stopwords):\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "tokens = list(filter(has_punct_or_stopword, tokens))\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('managers',),\n",
       " ('finally',),\n",
       " ('Report',),\n",
       " ('bringing', 'Myra'),\n",
       " ('legal', 'residence'),\n",
       " ('chicken',),\n",
       " ('expected',),\n",
       " ('every', 'opposing'),\n",
       " ('members',),\n",
       " ('board',),\n",
       " ('managed',),\n",
       " ('September',),\n",
       " ('putting',),\n",
       " ('We',),\n",
       " ('liquor',),\n",
       " ('years',),\n",
       " ('make',),\n",
       " ('people', 'might'),\n",
       " ('Caves',),\n",
       " ('denial',)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(tokens)[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep. So now let's built the vocabulary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-16 14:29:28,029 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2018-07-16 14:29:28,127 : INFO : adding document #10000 to Dictionary(3821 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:28,202 : INFO : adding document #20000 to Dictionary(6666 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:28,271 : INFO : adding document #30000 to Dictionary(9125 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:28,342 : INFO : adding document #40000 to Dictionary(11422 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:28,414 : INFO : adding document #50000 to Dictionary(13230 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:28,495 : INFO : adding document #60000 to Dictionary(14842 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:28,573 : INFO : adding document #70000 to Dictionary(16175 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:28,650 : INFO : adding document #80000 to Dictionary(17517 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:28,731 : INFO : adding document #90000 to Dictionary(19063 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:28,825 : INFO : adding document #100000 to Dictionary(20793 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:28,913 : INFO : adding document #110000 to Dictionary(22411 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:28,989 : INFO : adding document #120000 to Dictionary(23473 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:29,103 : INFO : adding document #130000 to Dictionary(24388 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:29,202 : INFO : adding document #140000 to Dictionary(25520 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:29,294 : INFO : adding document #150000 to Dictionary(27041 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:29,373 : INFO : adding document #160000 to Dictionary(28057 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:29,451 : INFO : adding document #170000 to Dictionary(28852 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:29,526 : INFO : adding document #180000 to Dictionary(29924 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:29,604 : INFO : adding document #190000 to Dictionary(30707 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:29,679 : INFO : adding document #200000 to Dictionary(31736 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:29,758 : INFO : adding document #210000 to Dictionary(32676 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:29,837 : INFO : adding document #220000 to Dictionary(33513 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:29,914 : INFO : adding document #230000 to Dictionary(34293 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:29,997 : INFO : adding document #240000 to Dictionary(35126 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:30,105 : INFO : adding document #250000 to Dictionary(35953 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:30,202 : INFO : adding document #260000 to Dictionary(36618 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:30,279 : INFO : adding document #270000 to Dictionary(37375 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:30,362 : INFO : adding document #280000 to Dictionary(38012 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:30,442 : INFO : adding document #290000 to Dictionary(38772 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:30,523 : INFO : adding document #300000 to Dictionary(39393 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:30,600 : INFO : adding document #310000 to Dictionary(40415 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:30,677 : INFO : adding document #320000 to Dictionary(40952 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:30,782 : INFO : adding document #330000 to Dictionary(41381 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:30,876 : INFO : adding document #340000 to Dictionary(41833 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:30,962 : INFO : adding document #350000 to Dictionary(42225 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:31,043 : INFO : adding document #360000 to Dictionary(42867 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:31,144 : INFO : adding document #370000 to Dictionary(43612 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:31,227 : INFO : adding document #380000 to Dictionary(44340 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:31,306 : INFO : adding document #390000 to Dictionary(44815 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:31,399 : INFO : adding document #400000 to Dictionary(45368 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:31,488 : INFO : adding document #410000 to Dictionary(45711 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:31,568 : INFO : adding document #420000 to Dictionary(46155 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:31,645 : INFO : adding document #430000 to Dictionary(46683 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:31,720 : INFO : adding document #440000 to Dictionary(47505 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:31,799 : INFO : adding document #450000 to Dictionary(48074 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:31,874 : INFO : adding document #460000 to Dictionary(48666 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:31,951 : INFO : adding document #470000 to Dictionary(49413 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:32,026 : INFO : adding document #480000 to Dictionary(49968 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:32,104 : INFO : adding document #490000 to Dictionary(50434 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:32,194 : INFO : adding document #500000 to Dictionary(50877 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:32,276 : INFO : adding document #510000 to Dictionary(51342 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:32,358 : INFO : adding document #520000 to Dictionary(51814 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:32,433 : INFO : adding document #530000 to Dictionary(52208 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:32,548 : INFO : adding document #540000 to Dictionary(52856 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:32,639 : INFO : adding document #550000 to Dictionary(53347 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:32,728 : INFO : adding document #560000 to Dictionary(53955 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:32,830 : INFO : adding document #570000 to Dictionary(54373 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:32,919 : INFO : adding document #580000 to Dictionary(54768 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-16 14:29:32,997 : INFO : adding document #590000 to Dictionary(55127 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:33,079 : INFO : adding document #600000 to Dictionary(55793 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:33,168 : INFO : adding document #610000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:33,254 : INFO : adding document #620000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:33,361 : INFO : adding document #630000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:33,449 : INFO : adding document #640000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:33,548 : INFO : adding document #650000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:33,641 : INFO : adding document #660000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:33,734 : INFO : adding document #670000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:33,826 : INFO : adding document #680000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:33,923 : INFO : adding document #690000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:34,014 : INFO : adding document #700000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:34,131 : INFO : adding document #710000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:34,234 : INFO : adding document #720000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:34,328 : INFO : adding document #730000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:34,419 : INFO : adding document #740000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:34,512 : INFO : adding document #750000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:34,615 : INFO : adding document #760000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:34,723 : INFO : adding document #770000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:34,820 : INFO : adding document #780000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:34,921 : INFO : adding document #790000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:35,013 : INFO : adding document #800000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:35,108 : INFO : adding document #810000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:35,205 : INFO : adding document #820000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:35,302 : INFO : adding document #830000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:35,396 : INFO : adding document #840000 to Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...)\n",
      "2018-07-16 14:29:35,438 : INFO : built Dictionary(55887 unique tokens: ['The', 'Fulton', 'County', 'Grand', 'Jury']...) from 844500 documents (total 1088313 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "dict = gensim.corpora.Dictionary(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Build a bag-of-words representation of the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to build our Tf-Idf model, we first need to build a bag-of-words representation for the brown corpus. This just means that we generate a table where the rows represent a document and the columns each token. The value for each cell, then, indicates the count of each token in each document. So let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the brown docs\n",
    "brown_docs = [brown.words(file_id) for file_id in brown.fileids()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(brown_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...],\n",
       " ['Austin', ',', 'Texas', '--', 'Committee', 'approval', ...]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_docs[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need tokenize the texts as we did above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.1 s, sys: 11.9 s, total: 33 s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def tokenize(text):\n",
    "    unigrams = [\" \".join(token) for token in list(ngrams(text, 1))]\n",
    "    bigrams = [\" \".join(token) for token in list (ngrams(text, 2))]\n",
    "    tokens = unigrams + bigrams\n",
    "    return tokens\n",
    "\n",
    "brown_docs_tokenized = [tokenize(text) for text in brown_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['promise .',\n",
       " 'State Welfare',\n",
       " 'we',\n",
       " 'over-all',\n",
       " 'directed',\n",
       " 'of',\n",
       " 'city',\n",
       " 'million to',\n",
       " 'work .',\n",
       " 'swipe',\n",
       " 'to',\n",
       " 'granted',\n",
       " 'the jury',\n",
       " 'employed',\n",
       " \"mayor's present\",\n",
       " 'will wind',\n",
       " 'his race',\n",
       " '.',\n",
       " 'the Legislature',\n",
       " 'Policeman']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle(brown_docs_tokenized[0])[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an array of tokenized documents, we can generate the bag-of-words representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_bow = [dict.doc2bow(doc) for doc in brown_docs_tokenized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look. If we examine the first document, we'll see that it contains a series of tuples with two integers. These, respectively, are the index of the word in the dict and the frequency with which that word occurs in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 28), (1, 14), (2, 10), (3, 1), (4, 1)]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_bow[0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking the first we can see that the index 0 refers to \"the\" so the occurs 28 times in the first document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Build the Tf-Idf model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point it is pretty trivial to generate the tf-idf model using gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-16 16:43:36,490 : INFO : collecting document frequencies\n",
      "2018-07-16 16:43:36,508 : INFO : PROGRESS: processing document #0\n",
      "2018-07-16 16:43:37,253 : INFO : calculating IDF weights for 500 documents and 55886 features (354853 matrix non-zeros)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 379 ms, sys: 275 ms, total: 654 ms\n",
      "Wall time: 1.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tfidf_model = gensim.models.TfidfModel(corpus_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we can save both the vocabulary and the model to be used in our keyword extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-07-16 16:45:00,363 : INFO : saving TfidfModel object under ./brown_tfidf.mm, separately None\n",
      "2018-07-16 16:45:01,252 : INFO : saved ./brown_tfidf.mm\n",
      "2018-07-16 16:45:01,254 : INFO : saving Dictionary object under ./brown_vocab.mm, separately None\n",
      "2018-07-16 16:45:01,756 : INFO : saved ./brown_vocab.mm\n"
     ]
    }
   ],
   "source": [
    "tfidf_model.save('./brown_tfidf.mm')\n",
    "dict.save('./brown_vocab.mm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
